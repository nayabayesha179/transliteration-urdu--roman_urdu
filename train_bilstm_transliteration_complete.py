# -*- coding: utf-8 -*-
"""Untitled13.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tQJon6Z0qD_DcUo8rvDUAWWVAgyadlaM
"""

import os
import sys
import re
import time
import math
import copy
import random
import zipfile
import unicodedata
import subprocess
from collections import Counter
from typing import List, Tuple

import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

# -----------------------------
# Configuration: pick experiment
# -----------------------------
EXP_ID = 0   # 0, 1 or 2 -> pick which experiment to run

# Candidate dataset zip paths (the environment where you uploaded file often stores it in /mnt/data)


CANDIDATE_ZIPS = [
    "./dataset.zip",
    "/content/dataset.zip",
    "/mnt/data/dataset.zip",
    "./urdu_ghazals_rekhta/dataset.zip"
]

# If dataset is not found locally, optionally clone the repo (requires git)
CLONE_REPO = True
REPO_URL = "https://github.com/amir9ume/urdu_ghazals_rekhta"
REPO_ZIP_URL = "https://github.com/amir9ume/urdu_ghazals_rekhta/archive/refs/heads/main.zip"

EXTRACT_TO = "./_rekhta_extracted"

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

SEED = 42
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)

# -----------------------------
# Helper: extract or clone dataset
# -----------------------------
def try_extract(zip_path, out_dir):
    try:
        with zipfile.ZipFile(zip_path, 'r') as z:
            z.extractall(out_dir)
        print("Extracted zip:", zip_path, "->", out_dir)
        return True
    except Exception as e:
        print("Failed to extract zip:", zip_path, e)
        return False

def try_clone(repo_url, out_dir):
    if os.path.exists(out_dir) and os.listdir(out_dir):
        print("Repo dir exists, skipping clone:", out_dir)
        return True
    try:
        print("Attempting git clone:", repo_url)
        subprocess.check_call(["git", "clone", repo_url, out_dir])
        return True
    except Exception as e:
        print("Git clone failed:", e)
        return False

def prepare_dataset_extraction():
    # look for candidate zips
    for p in CANDIDATE_ZIPS:
        if os.path.exists(p):
            os.makedirs(EXTRACT_TO, exist_ok=True)
            if try_extract(p, EXTRACT_TO):
                return EXTRACT_TO
    # try cloning
    if CLONE_REPO:
        ok = try_clone(REPO_URL, EXTRACT_TO)
        if ok:
            # many repos place files inside 'urdu_ghazals_rekhta' or 'urdu_ghazals_rekhta-main'
            return EXTRACT_TO
    # fallback: try to download zip
    try:
        import requests
        print("Downloading repo zip...")
        r = requests.get(REPO_ZIP_URL, stream=True, timeout=60)
        r.raise_for_status()
        tmp = "tmp_rekhta.zip"
        with open(tmp, "wb") as f:
            for chunk in r.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
        os.makedirs(EXTRACT_TO, exist_ok=True)
        try_extract(tmp, EXTRACT_TO)
        os.remove(tmp)
        return EXTRACT_TO
    except Exception as e:
        print("Download failed:", e)
    return None

# -----------------------------
# Discover Urdu lines in extracted dataset
# -----------------------------
def discover_urdu_lines(root_dir: str) -> List[str]:
    """
    Heuristics:
    - find folders that have 'ur' subfolders with files, read those
    - find files named ur.txt, urdu.txt, *.ur, etc
    - or find files containing Arabic script and read them
    """
    lines = []
    # 1) author/*/ur
    for d in os.listdir(root_dir):
        ap = os.path.join(root_dir, d)
        if not os.path.isdir(ap):
            continue
        ur_dir = os.path.join(ap, "ur")
        if os.path.isdir(ur_dir):
            for fname in os.listdir(ur_dir):
                fp = os.path.join(ur_dir, fname)
                if os.path.isfile(fp):
                    try:
                        with open(fp, 'r', encoding='utf-8', errors='ignore') as fh:
                            for ln in fh:
                                ln = ln.strip()
                                if ln:
                                    lines.append(ln)
                    except Exception:
                        continue
    if lines:
        print(f"Found {len(lines)} Urdu lines from author/ur folders.")
        return lines

    # 2) top-level filenames
    candidates = ['ur.txt','urdu.txt','urdu_lines.txt']
    for r, dirs, files in os.walk(root_dir):
        for f in files:
            if f.lower() in candidates:
                try:
                    with open(os.path.join(r,f), 'r', encoding='utf-8', errors='ignore') as fh:
                        for ln in fh:
                            ln=ln.strip()
                            if ln:
                                lines.append(ln)
                except Exception:
                    pass
    if lines:
        print("Found Urdu lines from top-level files.")
        return lines

    # 3) find files that contain Arabic script and read them
    for r, dirs, files in os.walk(root_dir):
        for f in files:
            fp = os.path.join(r,f)
            try:
                with open(fp, 'r', encoding='utf-8', errors='ignore') as fh:
                    text = fh.read(2000)
                if re.search(r'[\u0600-\u06FF]', text):
                    # read entire file
                    with open(fp, 'r', encoding='utf-8', errors='ignore') as fh:
                        for ln in fh:
                            ln = ln.strip()
                            if ln:
                                lines.append(ln)
            except Exception:
                continue
    if lines:
        print(f"Found {len(lines)} Urdu lines by scanning for Arabic script.")
        return lines

    print("No Urdu lines discovered in", root_dir)
    return lines

# -----------------------------
# Improved rule-based Romanizer
# -----------------------------
# This is a heuristic romanizer that tries to produce readable Roman Urdu from Urdu script.
# It's not perfect but much better than earlier naive mapping.
MAP = {
    # consonants
    "ب":"b","پ":"p","ت":"t","ٹ":"t","ث":"s","ج":"j","چ":"ch","ح":"h","خ":"kh",
    "د":"d","ڈ":"d","ذ":"z","ر":"r","ڑ":"r","ز":"z","ژ":"zh","س":"s","ش":"sh",
    "ص":"s","ض":"z","ط":"t","ظ":"z","ع":"","غ":"gh","ف":"f","ق":"q","ک":"k","گ":"g",
    "ل":"l","م":"m","ن":"n","ں":"n","ھ":"h","ہ":"h","ء":"'", "ؤ":"o",
    # vowels & semivowels
    "ا":"a","آ":"aa","إ":"i","أ":"a","ى":"a","ے":"e","ی":"y","ئ":"y","و":"o",
    "ِ":"i","ِّ":"i","ُ":"u","َ":"a","ٰ":"a",
    # punctuation / others handled below
}

# Extended handling: common ligatures/Arabic marks to remove
REMOVE = set(['\u0640','\ufeff','\u200f'])

def clean_urdu(s: str) -> str:
    if not s:
        return s
    s = unicodedata.normalize('NFC', s)
    for ch in REMOVE:
        s = s.replace(ch, '')
    s = re.sub(r'[“”«»"(){}\[\]*—–…·|<>†]', '', s)
    s = re.sub(r'\s+', ' ', s).strip()
    return s

def romanize_urdu(s: str) -> str:
    s = clean_urdu(s)
    out = []
    # simple algorithm:
    # - iterate characters, map using MAP
    # - if ascii keep ascii (numbers/punct)
    # - attempt to keep short words readable
    i = 0
    n = len(s)
    while i < n:
        ch = s[i]
        # skip punctuation
        if ch.isspace():
            out.append(' ')
            i += 1
            continue
        if ch in MAP:
            out.append(MAP[ch])
            i += 1
            continue
        # if ASCII (numbers or latin), keep it
        if ord(ch) < 128:
            out.append(ch)
            i += 1
            continue
        # fallback: try to decompose the character
        try:
            name = unicodedata.name(ch)
            out.append('')
        except Exception:
            out.append('')
        i += 1
    res = ''.join(out)
    # post-process: reduce repeated letters and fix some combos
    res = re.sub(r'\s+', ' ', res).strip()
    # simple vowel fixes
    res = re.sub(r'aa+', 'a', res)   # 'aa' -> 'a' (keeps shorter)
    res = re.sub(r'(^| )a ', r'\1', res)  # drop stray a as standalone? be conservative
    # typical roman usage: make words lower-case and tidy
    res = res.strip()
    res = res.lower()
    # final polish: common replacements
    res = re.sub(r'(^| )chh', r'\1ch', res)
    res = re.sub(r'([kg])h\1+', r'\1h', res)
    return res

# -----------------------------
# BPE from scratch (simple)
# -----------------------------
class BPE:
    def __init__(self, target_vocab_size=8000, min_freq=2):
        self.target_vocab_size = target_vocab_size
        self.min_freq = min_freq
        self.merges: List[Tuple[str,str]] = []
        self.vocab = {}   # token -> idx
        self.rev = {}

    def learn(self, texts: List[str]):
        # Build word-frequency with character sequences
        word_freq = Counter()
        for txt in texts:
            for w in txt.strip().split():
                if not w:
                    continue
                token = ' '.join(list(w)) + ' </w>'
                word_freq[token] += 1
        merges = []
        corpus = word_freq
        while True:
            pairs = Counter()
            for word, freq in corpus.items():
                symbols = word.split()
                for i in range(len(symbols)-1):
                    pairs[(symbols[i], symbols[i+1])] += freq
            if not pairs:
                break
            (a,b), freq = pairs.most_common(1)[0]
            if freq < self.min_freq:
                break
            merges.append((a,b))
            pat = re.escape(a + ' ' + b)
            new_corpus = {}
            for w,freqw in corpus.items():
                new_corpus[re.sub(pat, a+b, w)] = freqw
            corpus = new_corpus
            if len(merges) >= self.target_vocab_size:
                break
        self.merges = merges
        # build token list
        token_counter = Counter()
        for word, freq in corpus.items():
            for t in word.split():
                token_counter[t] += freq
        specials = ['<pad>','<sos>','<eos>','<unk>']
        tokens = specials + [t for t,_ in token_counter.most_common(self.target_vocab_size)]
        self.vocab = {t:i for i,t in enumerate(tokens)}
        self.rev = {i:t for t,i in self.vocab.items()}
        return self.vocab

    def encode_word(self, word: str) -> List[str]:
        symbols = list(word) + ['</w>']
        # apply merges greedily in learned order
        made_change = True
        while True:
            made = False
            for a,b in self.merges:
                i = 0
                while i < len(symbols)-1:
                    if symbols[i]==a and symbols[i+1]==b:
                        symbols[i] = a+b
                        del symbols[i+1]
                        made = True
                    else:
                        i += 1
            if not made:
                break
        if symbols and symbols[-1]=='</w>':
            symbols = symbols[:-1]
        return symbols

    def encode(self, text: str) -> List[int]:
        out=[]
        for w in text.strip().split():
            toks = self.encode_word(w)
            if not toks:
                out.append(self.vocab.get('<unk>'))
            else:
                for t in toks:
                    out.append(self.vocab.get(t, self.vocab.get('<unk>')))
        return out

# -----------------------------
# Dataset & collate
# -----------------------------
class ParallelDataset(Dataset):
    def __init__(self, pairs: List[Tuple[str,str]], src_bpe: BPE, trg_bpe: BPE, max_src=120, max_trg=140):
        self.pairs = pairs
        self.src_bpe = src_bpe
        self.trg_bpe = trg_bpe
        self.max_src = max_src
        self.max_trg = max_trg

    def __len__(self):
        return len(self.pairs)

    def __getitem__(self, idx):
        s,t = self.pairs[idx]
        s_ids = self.src_bpe.encode(s)[:self.max_src]
        t_ids = self.trg_bpe.encode(t)[:self.max_trg]
        return s_ids, t_ids

def collate_factory(src_pad, trg_pad, trg_sos, trg_eos, max_src, max_trg):
    def collate(batch):
        srcs, trgs = zip(*batch)
        B = len(srcs)
        max_s = min(max(len(x) for x in srcs), max_src)
        max_t = min(max(len(x) for x in trgs) + 2, max_trg)
        src_tensor = torch.full((B, max_s), src_pad, dtype=torch.long)
        trg_tensor = torch.full((B, max_t), trg_pad, dtype=torch.long)
        for i,(s,t) in enumerate(zip(srcs,trgs)):
            ls = min(len(s), max_s)
            lt = min(len(t), max_t-2)
            if ls>0:
                src_tensor[i,:ls] = torch.tensor(s[:ls], dtype=torch.long)
            seq = [trg_sos] + t[:lt] + [trg_eos]
            trg_tensor[i,:len(seq)] = torch.tensor(seq, dtype=torch.long)
        return src_tensor.to(DEVICE), trg_tensor.to(DEVICE)
    return collate

# -----------------------------
# Models
# -----------------------------
class Encoder(nn.Module):
    def __init__(self, input_dim, emb_dim, hid_dim, n_layers=2, dropout=0.1):
        super().__init__()
        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=0)
        self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=n_layers, dropout=dropout, batch_first=True, bidirectional=True)
        self.dropout = nn.Dropout(dropout)
    def forward(self, src):
        emb = self.dropout(self.embedding(src))
        outputs, (h,c) = self.rnn(emb)
        return outputs, (h,c)

class Decoder(nn.Module):
    def __init__(self, output_dim, emb_dim, hid_dim, n_layers=4, dropout=0.1):
        super().__init__()
        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=0)
        self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=n_layers, dropout=dropout, batch_first=True)
        self.fc = nn.Linear(hid_dim, output_dim)
        self.dropout = nn.Dropout(dropout)
    def forward(self, token_in, hidden):
        emb = self.dropout(self.embedding(token_in).unsqueeze(1))
        out, hidden = self.rnn(emb, hidden)
        logits = self.fc(out.squeeze(1))
        return logits, hidden

class Seq2Seq(nn.Module):
    def __init__(self, encoder: Encoder, decoder: Decoder, enc_hid, dec_hid):
        super().__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.h_map = nn.Linear(enc_hid*2, dec_hid)
        self.c_map = nn.Linear(enc_hid*2, dec_hid)
    def _reduce(self, h):
        n2,B,H = h.size()
        n = n2 // 2
        h = h.view(n, 2, B, H)
        hcat = torch.cat([h[:,0,:,:], h[:,1,:,:]], dim=2)
        mapped = torch.tanh(self.h_map(hcat))
        return mapped
    def forward(self, src, trg, teacher_forcing_ratio=0.5):
        B = src.size(0)
        T = trg.size(1)
        V = self.decoder.fc.out_features
        outputs = torch.zeros(B, T, V, device=src.device)
        enc_out, (h,c) = self.encoder(src)
        dec_h = self._reduce(h)
        dec_c = self._reduce(c)
        dec_need = self.decoder.rnn.num_layers
        if dec_h.size(0) < dec_need:
            last_h = dec_h[-1:].repeat(dec_need - dec_h.size(0), 1, 1)
            dec_h = torch.cat([dec_h, last_h], dim=0)
            last_c = dec_c[-1:].repeat(dec_need - dec_c.size(0), 1, 1)
            dec_c = torch.cat([dec_c, last_c], dim=0)
        hidden = (dec_h.contiguous(), dec_c.contiguous())
        input_tok = trg[:,0]
        for t in range(1, T):
            logits, hidden = self.decoder(input_tok, hidden)
            outputs[:,t] = logits
            top1 = logits.argmax(1)
            teacher_force = (random.random() < teacher_forcing_ratio)
            input_tok = trg[:,t] if teacher_force else top1
        return outputs

# -----------------------------
# Metrics: BLEU (sentence-level) & CER
# -----------------------------
def ngram_counts(tokens, n):
    return Counter(tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)) if len(tokens)>=n else Counter()

def sentence_bleu(ref_tokens, cand_tokens, max_n=4):
    precisions = []
    for n in range(1, max_n+1):
        ref_cnt = ngram_counts(ref_tokens, n)
        cand_cnt = ngram_counts(cand_tokens, n)
        overlap = sum(min(cand_cnt[ng], ref_cnt.get(ng,0)) for ng in cand_cnt)
        total = sum(cand_cnt.values())
        p = 0.0 if total == 0 else overlap/total
        precisions.append(p)
    if min(precisions) == 0:
        geo = 0.0
    else:
        geo = math.exp(sum(math.log(p) for p in precisions)/len(precisions))
    rlen = len(ref_tokens); clen = len(cand_tokens)
    if clen == 0:
        bp = 0.0
    else:
        bp = 1.0 if clen > rlen else math.exp(1 - rlen/clen)
    return bp * geo

def cer(ref, hyp):
    a=list(ref); b=list(hyp)
    n=len(a); m=len(b)
    if n==0:
        return float(m)
    dp=[[0]*(m+1) for _ in range(n+1)]
    for i in range(n+1): dp[i][0]=i
    for j in range(m+1): dp[0][j]=j
    for i in range(1,n+1):
        for j in range(1,m+1):
            cost = 0 if a[i-1]==b[j-1] else 1
            dp[i][j] = min(dp[i-1][j]+1, dp[i][j-1]+1, dp[i-1][j-1]+cost)
    return dp[n][m]/max(1,n)

# -----------------------------
# Training & evaluation
# -----------------------------
def train_epoch(model, loader, optimizer, criterion, clip, tf_ratio):
    model.train()
    total_loss = 0.0
    for src, trg in loader:
        optimizer.zero_grad()
        out = model(src, trg, teacher_forcing_ratio=tf_ratio)
        V = out.shape[-1]
        out_flat = out[:,1:,:].contiguous().view(-1, V)
        trg_flat = trg[:,1:].contiguous().view(-1)
        loss = criterion(out_flat, trg_flat)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
        optimizer.step()
        total_loss += loss.item()
    return total_loss / max(1, len(loader))

def evaluate(model, loader, criterion, trg_rev_vocab):
    model.eval()
    total_loss = 0.0
    refs=[]; hyps=[]
    with torch.no_grad():
        for src, trg in loader:
            outputs = model(src, trg, teacher_forcing_ratio=0.0)
            V = outputs.shape[-1]
            out_flat = outputs[:,1:,:].contiguous().view(-1, V)
            trg_flat = trg[:,1:].contiguous().view(-1)
            loss = criterion(out_flat, trg_flat)
            total_loss += loss.item()
            B = src.size(0)
            for i in range(B):
                # reference
                ref_ids = trg[i].cpu().tolist()
                ref_tokens=[]
                for idx in ref_ids:
                    if idx==trg_pad or idx==trg_sos:
                        continue
                    if idx==trg_eos:
                        break
                    ref_tokens.append(trg_rev_vocab.get(idx,'<unk>'))
                refs.append(' '.join(ref_tokens))
                # hypothesis from outputs
                hyp_ids = outputs[i].argmax(1).cpu().tolist()
                hyp_tokens=[]
                for idx in hyp_ids:
                    if idx==trg_pad or idx==trg_sos:
                        continue
                    if idx==trg_eos:
                        break
                    hyp_tokens.append(trg_rev_vocab.get(idx,'<unk>'))
                hyps.append(' '.join(hyp_tokens))
    avg_loss = total_loss / max(1, len(loader))
    return avg_loss, refs, hyps

# -----------------------------
# Main pipeline
# -----------------------------
def main():
    start_time = time.time()
    # prepare dataset extraction
    extracted_root = prepare_dataset_extraction()
    if extracted_root is None:
        print("Could not prepare dataset. Put dataset.zip in one of candidate paths or enable cloning.")
        sys.exit(1)

    # try to find dataset root folder containing 'dataset' subfolder
    dataset_root = None
    # common structure: some zips extract to "urdu_ghazals_rekhta-main/dataset" or "urdu_ghazals_rekhta/dataset"
    for cand in [os.path.join(extracted_root,"dataset"), os.path.join(extracted_root,"urdu_ghazals_rekhta","dataset"), extracted_root]:
        if os.path.isdir(cand):
            # if it's the top-level extracted dir, check children for 'dataset'
            if os.path.basename(cand).lower() == "dataset" or any('dataset' in d.lower() for d in os.listdir(os.path.dirname(cand)) if os.path.isdir(os.path.dirname(cand))):
                dataset_root = cand
                break
            else:
                dataset_root = cand
                break
    if dataset_root is None:
        dataset_root = extracted_root
    print("Using dataset root:", dataset_root)

    # discover Urdu lines
    urdu_lines = discover_urdu_lines(dataset_root)
    if not urdu_lines:
        print("No Urdu lines discovered. Inspect dataset directory and try again.")
        sys.exit(1)

    # LIMIT for speed (optional). For now use all lines.
    print("Total raw Urdu lines found:", len(urdu_lines))

    # generate roman targets (fully generated approach)
    pairs = []
    for u in urdu_lines:
        u_clean = clean_urdu(u)
        roman = romanize_urdu(u_clean)
        if roman and u_clean:
            pairs.append((u_clean.lower(), roman.lower()))

    print("Total synthetic pairs generated:", len(pairs))
    if len(pairs) < 20:
        print("Warning: too few synthetic pairs:", len(pairs))

    # shuffle and split 50/25/25
    random.shuffle(pairs)
    N = len(pairs)
    n_train = int(0.5 * N)
    n_val = int(0.25 * N)
    train_pairs = pairs[:n_train]
    val_pairs = pairs[n_train:n_train+n_val]
    test_pairs = pairs[n_train+n_val:]
    print("Splits (train/val/test):", len(train_pairs), len(val_pairs), len(test_pairs))

    # learn BPE on train side (both source and target)
    print("Learning BPE (source)...")
    src_texts = [s for s,t in train_pairs]
    trg_texts = [t for s,t in train_pairs]
    src_bpe = BPE(target_vocab_size=cfg['bpe_vocab'], min_freq=cfg.get('min_bpe_freq',2))
    trg_bpe = BPE(target_vocab_size=cfg['bpe_vocab'], min_freq=cfg.get('min_bpe_freq',2))
    src_bpe.learn(src_texts)
    trg_bpe.learn(trg_texts)
    print("Source vocab size:", len(src_bpe.vocab), "Target vocab size:", len(trg_bpe.vocab))

    # prepare datasets & dataloaders
    global trg_pad, trg_sos, trg_eos
    trg_pad = trg_bpe.vocab['<pad>']; trg_sos = trg_bpe.vocab['<sos>']; trg_eos = trg_bpe.vocab['<eos>']
    src_pad = src_bpe.vocab['<pad>']

    train_ds = ParallelDataset(train_pairs, src_bpe, trg_bpe, max_src=cfg['max_src_len'], max_trg=cfg['max_trg_len'])
    val_ds = ParallelDataset(val_pairs, src_bpe, trg_bpe, max_src=cfg['max_src_len'], max_trg=cfg['max_trg_len'])
    test_ds = ParallelDataset(test_pairs, src_bpe, trg_bpe, max_src=cfg['max_src_len'], max_trg=cfg['max_trg_len'])

    collate = collate_factory(src_pad, trg_pad, trg_sos, trg_eos, cfg['max_src_len'], cfg['max_trg_len'])
    train_loader = DataLoader(train_ds, batch_size=cfg['batch_size'], shuffle=True, collate_fn=collate)
    val_loader = DataLoader(val_ds, batch_size=cfg['batch_size'], shuffle=False, collate_fn=collate)
    test_loader = DataLoader(test_ds, batch_size=cfg['batch_size'], shuffle=False, collate_fn=collate)

    # build model
    encoder = Encoder(len(src_bpe.vocab), cfg['emb_dim'], cfg['enc_hidden'], n_layers=cfg['enc_layers'], dropout=cfg['dropout']).to(DEVICE)
    decoder = Decoder(len(trg_bpe.vocab), cfg['emb_dim'], cfg['dec_hidden'], n_layers=cfg['dec_layers'], dropout=cfg['dropout']).to(DEVICE)
    model = Seq2Seq(encoder, decoder, cfg['enc_hidden'], cfg['dec_hidden']).to(DEVICE)

    optimizer = torch.optim.Adam(model.parameters(), lr=cfg['lr'], weight_decay=cfg.get('weight_decay',0.0))
    criterion = nn.CrossEntropyLoss(ignore_index=trg_pad)

    print("Training on device:", DEVICE)
    best_val = float('inf'); best_state=None; epochs_no_improve=0
    for epoch in range(1, cfg['epochs']+1):
        # linear teacher forcing anneal
        if cfg['epochs']>1:
            frac = (epoch-1)/(cfg['epochs']-1)
            tf = cfg['teacher_forcing_start'] + (cfg['teacher_forcing_end'] - cfg['teacher_forcing_start']) * frac
        else:
            tf = cfg['teacher_forcing_end']
        t0 = time.time()
        train_loss = train_epoch(model, train_loader, optimizer, criterion, cfg['grad_clip'], tf)
        val_loss, val_refs, val_hyps = evaluate(model, val_loader, criterion, trg_bpe.rev)
        t1 = time.time()
        # compute val CER and BLEU averages (sentence-level average)
        val_bleu = 0.0
        val_cer = 0.0
        if val_refs:
            val_bleu = sum(sentence_bleu(r.split(), h.split()) for r,h in zip(val_refs, val_hyps)) / len(val_refs)
            val_cer = sum(cer(r.replace(' ','') , h.replace(' ','')) for r,h in zip(val_refs, val_hyps)) / len(val_refs)
        print(f"[{cfg['name']}] Epoch {epoch}/{cfg['epochs']} TrainLoss={train_loss:.4f} ValLoss={val_loss:.4f} ValBLEU={val_bleu:.4f} ValCER={val_cer:.4f} TF={tf:.3f} time={t1-t0:.1f}s")
        if val_loss < best_val - 1e-6:
            best_val = val_loss
            best_state = copy.deepcopy(model.state_dict())
            torch.save(best_state, f"{cfg['name']}_best.pt")
            print("Saved checkpoint:", f"{cfg['name']}_best.pt")
            epochs_no_improve = 0
        else:
            epochs_no_improve += 1
            if epochs_no_improve >= cfg['patience']:
                print("Early stopping.")
                break

    # final evaluation on test
    if best_state is not None:
        model.load_state_dict(best_state)
    test_loss, test_refs, test_hyps = evaluate(model, test_loader, criterion, trg_bpe.rev)
    test_bleu = 0.0; test_cer=0.0
    if test_refs:
        test_bleu = sum(sentence_bleu(r.split(), h.split()) for r,h in zip(test_refs, test_hyps)) / len(test_refs)
        test_cer = sum(cer(r.replace(' ','') , h.replace(' ','')) for r,h in zip(test_refs, test_hyps)) / len(test_refs)
    ppl = math.exp(test_loss) if test_loss < 100 else float('inf')

    print("\n=== TEST RESULTS ===")
    print("Test loss:", test_loss)
    print("Test Perplexity:", ppl)
    print("Test BLEU:", test_bleu)
    print("Test CER:", test_cer)

    # qualitative examples: show a few source -> expected (generated) -> predicted
    n_show = min(12, len(test_refs))
    print("\n--- Sample Predictions ---")
    for i in range(n_show):
        src_line = test_pairs[i][0]
        ref_line = test_refs[i]
        hyp_line = test_hyps[i]
        print("Input: ", src_line)
        print("True:  ", ref_line)
        print("Pred:  ", hyp_line)
        print("CER:   ", round(cer(ref_line.replace(' ','') , hyp_line.replace(' ','')), 4))
        print("-"*40)

    total_time = time.time() - start_time
    print("Total time (s):", round(total_time,1))

if __name__ == "__main__":
    main()

# -----------------------------
# Experiments (three configs)
# -----------------------------
experiments = [
    {   # small
        "name":"exp_small",
        "emb_dim":128,
        "enc_hidden":256,
        "enc_layers":2,
        "dec_hidden":256,
        "dec_layers":4,
        "dropout":0.3,
        "lr":5e-4,
        "batch_size":64,
        "epochs":30,
        "patience":5,
        "max_src_len":120,
        "max_trg_len":140,
        "bpe_vocab":4000,
        "min_bpe_freq":2,
        "grad_clip":1.0,
        "teacher_forcing_start":0.9,
        "teacher_forcing_end":0.3,
    },
    {   # medium
        "name":"exp_med",
        "emb_dim":256,
        "enc_hidden":512,
        "enc_layers":3,
        "dec_hidden":512,
        "dec_layers":4,
        "dropout":0.5,
        "lr":1e-4,
        "batch_size":32,
        "epochs":30,
        "patience":5,
        "max_src_len":120,
        "max_trg_len":140,
        "bpe_vocab":8000,
        "min_bpe_freq":2,
        "grad_clip":1.0,
        "teacher_forcing_start":0.9,
        "teacher_forcing_end":0.3,
    },
    {   # large
        "name":"exp_large",
        "emb_dim":512,
        "enc_hidden":512,
        "enc_layers":4,
        "dec_hidden":512,
        "dec_layers":4,
        "dropout":0.1,
        "lr":1e-3,
        "batch_size":128,
        "epochs":20,
        "patience":5,
        "max_src_len":120,
        "max_trg_len":140,
        "bpe_vocab":12000,
        "min_bpe_freq":2,
        "grad_clip":1.0,
        "teacher_forcing_start":0.9,
        "teacher_forcing_end":0.3,
    }
]

cfg = experiments[EXP_ID]
print("Selected experiment:", EXP_ID, cfg["name"])
print(cfg)
