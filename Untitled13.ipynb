{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3l4Fp0lOeTodJxM/RuOP8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nayabayesha179/transliteration-urdu--roman_urdu/blob/main/Untitled13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import re\n",
        "import time\n",
        "import math\n",
        "import copy\n",
        "import random\n",
        "import zipfile\n",
        "import unicodedata\n",
        "import subprocess\n",
        "from collections import Counter\n",
        "from typing import List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "oAme-IFSfibW"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCjYb8j1TmbM",
        "outputId": "099ace21-62d4-4014-d8ae-f1de3dfeeb55"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7e65a7f66e70>"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "# -----------------------------\n",
        "# Configuration: pick experiment\n",
        "# -----------------------------\n",
        "EXP_ID = 0   # 0, 1 or 2 -> pick which experiment to run\n",
        "\n",
        "# Candidate dataset zip paths (the environment where you uploaded file often stores it in /mnt/data)\n",
        "\n",
        "\n",
        "CANDIDATE_ZIPS = [\n",
        "    \"./dataset.zip\",\n",
        "    \"/content/dataset.zip\",\n",
        "    \"/mnt/data/dataset.zip\",\n",
        "    \"./urdu_ghazals_rekhta/dataset.zip\"\n",
        "]\n",
        "\n",
        "# If dataset is not found locally, optionally clone the repo (requires git)\n",
        "CLONE_REPO = True\n",
        "REPO_URL = \"https://github.com/amir9ume/urdu_ghazals_rekhta\"\n",
        "REPO_ZIP_URL = \"https://github.com/amir9ume/urdu_ghazals_rekhta/archive/refs/heads/main.zip\"\n",
        "\n",
        "EXTRACT_TO = \"./_rekhta_extracted\"\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Experiments (three configs)\n",
        "# -----------------------------\n",
        "experiments = [\n",
        "    {   # small\n",
        "        \"name\":\"exp_small\",\n",
        "        \"emb_dim\":128,\n",
        "        \"enc_hidden\":256,\n",
        "        \"enc_layers\":2,\n",
        "        \"dec_hidden\":256,\n",
        "        \"dec_layers\":4,\n",
        "        \"dropout\":0.3,\n",
        "        \"lr\":5e-4,\n",
        "        \"batch_size\":64,\n",
        "        \"epochs\":30,\n",
        "        \"patience\":5,\n",
        "        \"max_src_len\":120,\n",
        "        \"max_trg_len\":140,\n",
        "        \"bpe_vocab\":4000,\n",
        "        \"min_bpe_freq\":2,\n",
        "        \"grad_clip\":1.0,\n",
        "        \"teacher_forcing_start\":0.9,\n",
        "        \"teacher_forcing_end\":0.3,\n",
        "    },\n",
        "    {   # medium\n",
        "        \"name\":\"exp_med\",\n",
        "        \"emb_dim\":256,\n",
        "        \"enc_hidden\":512,\n",
        "        \"enc_layers\":3,\n",
        "        \"dec_hidden\":512,\n",
        "        \"dec_layers\":4,\n",
        "        \"dropout\":0.5,\n",
        "        \"lr\":1e-4,\n",
        "        \"batch_size\":32,\n",
        "        \"epochs\":30,\n",
        "        \"patience\":5,\n",
        "        \"max_src_len\":120,\n",
        "        \"max_trg_len\":140,\n",
        "        \"bpe_vocab\":8000,\n",
        "        \"min_bpe_freq\":2,\n",
        "        \"grad_clip\":1.0,\n",
        "        \"teacher_forcing_start\":0.9,\n",
        "        \"teacher_forcing_end\":0.3,\n",
        "    },\n",
        "    {   # large\n",
        "        \"name\":\"exp_large\",\n",
        "        \"emb_dim\":512,\n",
        "        \"enc_hidden\":512,\n",
        "        \"enc_layers\":4,\n",
        "        \"dec_hidden\":512,\n",
        "        \"dec_layers\":4,\n",
        "        \"dropout\":0.1,\n",
        "        \"lr\":1e-3,\n",
        "        \"batch_size\":128,\n",
        "        \"epochs\":20,\n",
        "        \"patience\":5,\n",
        "        \"max_src_len\":120,\n",
        "        \"max_trg_len\":140,\n",
        "        \"bpe_vocab\":12000,\n",
        "        \"min_bpe_freq\":2,\n",
        "        \"grad_clip\":1.0,\n",
        "        \"teacher_forcing_start\":0.9,\n",
        "        \"teacher_forcing_end\":0.3,\n",
        "    }\n",
        "]\n",
        "\n",
        "cfg = experiments[EXP_ID]\n",
        "print(\"Selected experiment:\", EXP_ID, cfg[\"name\"])\n",
        "print(cfg)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0HEUZf1m4dZ",
        "outputId": "30d42781-43f9-4dbb-89a4-2e5005b5ee0b"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected experiment: 0 exp_small\n",
            "{'name': 'exp_small', 'emb_dim': 128, 'enc_hidden': 256, 'enc_layers': 2, 'dec_hidden': 256, 'dec_layers': 4, 'dropout': 0.3, 'lr': 0.0005, 'batch_size': 64, 'epochs': 30, 'patience': 5, 'max_src_len': 120, 'max_trg_len': 140, 'bpe_vocab': 4000, 'min_bpe_freq': 2, 'grad_clip': 1.0, 'teacher_forcing_start': 0.9, 'teacher_forcing_end': 0.3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Helper: extract or clone dataset\n",
        "# -----------------------------\n",
        "def try_extract(zip_path, out_dir):\n",
        "    try:\n",
        "        with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "            z.extractall(out_dir)\n",
        "        print(\"Extracted zip:\", zip_path, \"->\", out_dir)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(\"Failed to extract zip:\", zip_path, e)\n",
        "        return False\n",
        "\n",
        "def try_clone(repo_url, out_dir):\n",
        "    if os.path.exists(out_dir) and os.listdir(out_dir):\n",
        "        print(\"Repo dir exists, skipping clone:\", out_dir)\n",
        "        return True\n",
        "    try:\n",
        "        print(\"Attempting git clone:\", repo_url)\n",
        "        subprocess.check_call([\"git\", \"clone\", repo_url, out_dir])\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(\"Git clone failed:\", e)\n",
        "        return False\n",
        "\n",
        "def prepare_dataset_extraction():\n",
        "    # look for candidate zips\n",
        "    for p in CANDIDATE_ZIPS:\n",
        "        if os.path.exists(p):\n",
        "            os.makedirs(EXTRACT_TO, exist_ok=True)\n",
        "            if try_extract(p, EXTRACT_TO):\n",
        "                return EXTRACT_TO\n",
        "    # try cloning\n",
        "    if CLONE_REPO:\n",
        "        ok = try_clone(REPO_URL, EXTRACT_TO)\n",
        "        if ok:\n",
        "            # many repos place files inside 'urdu_ghazals_rekhta' or 'urdu_ghazals_rekhta-main'\n",
        "            return EXTRACT_TO\n",
        "    # fallback: try to download zip\n",
        "    try:\n",
        "        import requests\n",
        "        print(\"Downloading repo zip...\")\n",
        "        r = requests.get(REPO_ZIP_URL, stream=True, timeout=60)\n",
        "        r.raise_for_status()\n",
        "        tmp = \"tmp_rekhta.zip\"\n",
        "        with open(tmp, \"wb\") as f:\n",
        "            for chunk in r.iter_content(chunk_size=8192):\n",
        "                if chunk:\n",
        "                    f.write(chunk)\n",
        "        os.makedirs(EXTRACT_TO, exist_ok=True)\n",
        "        try_extract(tmp, EXTRACT_TO)\n",
        "        os.remove(tmp)\n",
        "        return EXTRACT_TO\n",
        "    except Exception as e:\n",
        "        print(\"Download failed:\", e)\n",
        "    return None"
      ],
      "metadata": {
        "id": "NujIeZgidfrO"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Discover Urdu lines in extracted dataset\n",
        "# -----------------------------\n",
        "def discover_urdu_lines(root_dir: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    Heuristics:\n",
        "    - find folders that have 'ur' subfolders with files, read those\n",
        "    - find files named ur.txt, urdu.txt, *.ur, etc\n",
        "    - or find files containing Arabic script and read them\n",
        "    \"\"\"\n",
        "    lines = []\n",
        "    # 1) author/*/ur\n",
        "    for d in os.listdir(root_dir):\n",
        "        ap = os.path.join(root_dir, d)\n",
        "        if not os.path.isdir(ap):\n",
        "            continue\n",
        "        ur_dir = os.path.join(ap, \"ur\")\n",
        "        if os.path.isdir(ur_dir):\n",
        "            for fname in os.listdir(ur_dir):\n",
        "                fp = os.path.join(ur_dir, fname)\n",
        "                if os.path.isfile(fp):\n",
        "                    try:\n",
        "                        with open(fp, 'r', encoding='utf-8', errors='ignore') as fh:\n",
        "                            for ln in fh:\n",
        "                                ln = ln.strip()\n",
        "                                if ln:\n",
        "                                    lines.append(ln)\n",
        "                    except Exception:\n",
        "                        continue\n",
        "    if lines:\n",
        "        print(f\"Found {len(lines)} Urdu lines from author/ur folders.\")\n",
        "        return lines\n",
        "\n",
        "    # 2) top-level filenames\n",
        "    candidates = ['ur.txt','urdu.txt','urdu_lines.txt']\n",
        "    for r, dirs, files in os.walk(root_dir):\n",
        "        for f in files:\n",
        "            if f.lower() in candidates:\n",
        "                try:\n",
        "                    with open(os.path.join(r,f), 'r', encoding='utf-8', errors='ignore') as fh:\n",
        "                        for ln in fh:\n",
        "                            ln=ln.strip()\n",
        "                            if ln:\n",
        "                                lines.append(ln)\n",
        "                except Exception:\n",
        "                    pass\n",
        "    if lines:\n",
        "        print(\"Found Urdu lines from top-level files.\")\n",
        "        return lines\n",
        "\n",
        "    # 3) find files that contain Arabic script and read them\n",
        "    for r, dirs, files in os.walk(root_dir):\n",
        "        for f in files:\n",
        "            fp = os.path.join(r,f)\n",
        "            try:\n",
        "                with open(fp, 'r', encoding='utf-8', errors='ignore') as fh:\n",
        "                    text = fh.read(2000)\n",
        "                if re.search(r'[\\u0600-\\u06FF]', text):\n",
        "                    # read entire file\n",
        "                    with open(fp, 'r', encoding='utf-8', errors='ignore') as fh:\n",
        "                        for ln in fh:\n",
        "                            ln = ln.strip()\n",
        "                            if ln:\n",
        "                                lines.append(ln)\n",
        "            except Exception:\n",
        "                continue\n",
        "    if lines:\n",
        "        print(f\"Found {len(lines)} Urdu lines by scanning for Arabic script.\")\n",
        "        return lines\n",
        "\n",
        "    print(\"No Urdu lines discovered in\", root_dir)\n",
        "    return lines"
      ],
      "metadata": {
        "id": "yKs2JwNudmfU"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Improved rule-based Romanizer\n",
        "# -----------------------------\n",
        "# This is a heuristic romanizer that tries to produce readable Roman Urdu from Urdu script.\n",
        "# It's not perfect but much better than earlier naive mapping.\n",
        "MAP = {\n",
        "    # consonants\n",
        "    \"ب\":\"b\",\"پ\":\"p\",\"ت\":\"t\",\"ٹ\":\"t\",\"ث\":\"s\",\"ج\":\"j\",\"چ\":\"ch\",\"ح\":\"h\",\"خ\":\"kh\",\n",
        "    \"د\":\"d\",\"ڈ\":\"d\",\"ذ\":\"z\",\"ر\":\"r\",\"ڑ\":\"r\",\"ز\":\"z\",\"ژ\":\"zh\",\"س\":\"s\",\"ش\":\"sh\",\n",
        "    \"ص\":\"s\",\"ض\":\"z\",\"ط\":\"t\",\"ظ\":\"z\",\"ع\":\"\",\"غ\":\"gh\",\"ف\":\"f\",\"ق\":\"q\",\"ک\":\"k\",\"گ\":\"g\",\n",
        "    \"ل\":\"l\",\"م\":\"m\",\"ن\":\"n\",\"ں\":\"n\",\"ھ\":\"h\",\"ہ\":\"h\",\"ء\":\"'\", \"ؤ\":\"o\",\n",
        "    # vowels & semivowels\n",
        "    \"ا\":\"a\",\"آ\":\"aa\",\"إ\":\"i\",\"أ\":\"a\",\"ى\":\"a\",\"ے\":\"e\",\"ی\":\"y\",\"ئ\":\"y\",\"و\":\"o\",\n",
        "    \"ِ\":\"i\",\"ِّ\":\"i\",\"ُ\":\"u\",\"َ\":\"a\",\"ٰ\":\"a\",\n",
        "    # punctuation / others handled below\n",
        "}\n",
        "\n",
        "# Extended handling: common ligatures/Arabic marks to remove\n",
        "REMOVE = set(['\\u0640','\\ufeff','\\u200f'])\n",
        "\n",
        "def clean_urdu(s: str) -> str:\n",
        "    if not s:\n",
        "        return s\n",
        "    s = unicodedata.normalize('NFC', s)\n",
        "    for ch in REMOVE:\n",
        "        s = s.replace(ch, '')\n",
        "    s = re.sub(r'[“”«»\"(){}\\[\\]*—–…·|<>†]', '', s)\n",
        "    s = re.sub(r'\\s+', ' ', s).strip()\n",
        "    return s\n",
        "\n",
        "def romanize_urdu(s: str) -> str:\n",
        "    s = clean_urdu(s)\n",
        "    out = []\n",
        "    # simple algorithm:\n",
        "    # - iterate characters, map using MAP\n",
        "    # - if ascii keep ascii (numbers/punct)\n",
        "    # - attempt to keep short words readable\n",
        "    i = 0\n",
        "    n = len(s)\n",
        "    while i < n:\n",
        "        ch = s[i]\n",
        "        # skip punctuation\n",
        "        if ch.isspace():\n",
        "            out.append(' ')\n",
        "            i += 1\n",
        "            continue\n",
        "        if ch in MAP:\n",
        "            out.append(MAP[ch])\n",
        "            i += 1\n",
        "            continue\n",
        "        # if ASCII (numbers or latin), keep it\n",
        "        if ord(ch) < 128:\n",
        "            out.append(ch)\n",
        "            i += 1\n",
        "            continue\n",
        "        # fallback: try to decompose the character\n",
        "        try:\n",
        "            name = unicodedata.name(ch)\n",
        "            out.append('')\n",
        "        except Exception:\n",
        "            out.append('')\n",
        "        i += 1\n",
        "    res = ''.join(out)\n",
        "    # post-process: reduce repeated letters and fix some combos\n",
        "    res = re.sub(r'\\s+', ' ', res).strip()\n",
        "    # simple vowel fixes\n",
        "    res = re.sub(r'aa+', 'a', res)   # 'aa' -> 'a' (keeps shorter)\n",
        "    res = re.sub(r'(^| )a ', r'\\1', res)  # drop stray a as standalone? be conservative\n",
        "    # typical roman usage: make words lower-case and tidy\n",
        "    res = res.strip()\n",
        "    res = res.lower()\n",
        "    # final polish: common replacements\n",
        "    res = re.sub(r'(^| )chh', r'\\1ch', res)\n",
        "    res = re.sub(r'([kg])h\\1+', r'\\1h', res)\n",
        "    return res\n"
      ],
      "metadata": {
        "id": "u7v9EcKGdrlz"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# BPE from scratch (simple)\n",
        "# -----------------------------\n",
        "class BPE:\n",
        "    def __init__(self, target_vocab_size=8000, min_freq=2):\n",
        "        self.target_vocab_size = target_vocab_size\n",
        "        self.min_freq = min_freq\n",
        "        self.merges: List[Tuple[str,str]] = []\n",
        "        self.vocab = {}   # token -> idx\n",
        "        self.rev = {}\n",
        "\n",
        "    def learn(self, texts: List[str]):\n",
        "        # Build word-frequency with character sequences\n",
        "        word_freq = Counter()\n",
        "        for txt in texts:\n",
        "            for w in txt.strip().split():\n",
        "                if not w:\n",
        "                    continue\n",
        "                token = ' '.join(list(w)) + ' </w>'\n",
        "                word_freq[token] += 1\n",
        "        merges = []\n",
        "        corpus = word_freq\n",
        "        while True:\n",
        "            pairs = Counter()\n",
        "            for word, freq in corpus.items():\n",
        "                symbols = word.split()\n",
        "                for i in range(len(symbols)-1):\n",
        "                    pairs[(symbols[i], symbols[i+1])] += freq\n",
        "            if not pairs:\n",
        "                break\n",
        "            (a,b), freq = pairs.most_common(1)[0]\n",
        "            if freq < self.min_freq:\n",
        "                break\n",
        "            merges.append((a,b))\n",
        "            pat = re.escape(a + ' ' + b)\n",
        "            new_corpus = {}\n",
        "            for w,freqw in corpus.items():\n",
        "                new_corpus[re.sub(pat, a+b, w)] = freqw\n",
        "            corpus = new_corpus\n",
        "            if len(merges) >= self.target_vocab_size:\n",
        "                break\n",
        "        self.merges = merges\n",
        "        # build token list\n",
        "        token_counter = Counter()\n",
        "        for word, freq in corpus.items():\n",
        "            for t in word.split():\n",
        "                token_counter[t] += freq\n",
        "        specials = ['<pad>','<sos>','<eos>','<unk>']\n",
        "        tokens = specials + [t for t,_ in token_counter.most_common(self.target_vocab_size)]\n",
        "        self.vocab = {t:i for i,t in enumerate(tokens)}\n",
        "        self.rev = {i:t for t,i in self.vocab.items()}\n",
        "        return self.vocab\n",
        "\n",
        "    def encode_word(self, word: str) -> List[str]:\n",
        "        symbols = list(word) + ['</w>']\n",
        "        # apply merges greedily in learned order\n",
        "        made_change = True\n",
        "        while True:\n",
        "            made = False\n",
        "            for a,b in self.merges:\n",
        "                i = 0\n",
        "                while i < len(symbols)-1:\n",
        "                    if symbols[i]==a and symbols[i+1]==b:\n",
        "                        symbols[i] = a+b\n",
        "                        del symbols[i+1]\n",
        "                        made = True\n",
        "                    else:\n",
        "                        i += 1\n",
        "            if not made:\n",
        "                break\n",
        "        if symbols and symbols[-1]=='</w>':\n",
        "            symbols = symbols[:-1]\n",
        "        return symbols\n",
        "\n",
        "    def encode(self, text: str) -> List[int]:\n",
        "        out=[]\n",
        "        for w in text.strip().split():\n",
        "            toks = self.encode_word(w)\n",
        "            if not toks:\n",
        "                out.append(self.vocab.get('<unk>'))\n",
        "            else:\n",
        "                for t in toks:\n",
        "                    out.append(self.vocab.get(t, self.vocab.get('<unk>')))\n",
        "        return out"
      ],
      "metadata": {
        "id": "Igpp9Vp9dwfG"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Dataset & collate\n",
        "# -----------------------------\n",
        "class ParallelDataset(Dataset):\n",
        "    def __init__(self, pairs: List[Tuple[str,str]], src_bpe: BPE, trg_bpe: BPE, max_src=120, max_trg=140):\n",
        "        self.pairs = pairs\n",
        "        self.src_bpe = src_bpe\n",
        "        self.trg_bpe = trg_bpe\n",
        "        self.max_src = max_src\n",
        "        self.max_trg = max_trg\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        s,t = self.pairs[idx]\n",
        "        s_ids = self.src_bpe.encode(s)[:self.max_src]\n",
        "        t_ids = self.trg_bpe.encode(t)[:self.max_trg]\n",
        "        return s_ids, t_ids\n",
        "\n",
        "def collate_factory(src_pad, trg_pad, trg_sos, trg_eos, max_src, max_trg):\n",
        "    def collate(batch):\n",
        "        srcs, trgs = zip(*batch)\n",
        "        B = len(srcs)\n",
        "        max_s = min(max(len(x) for x in srcs), max_src)\n",
        "        max_t = min(max(len(x) for x in trgs) + 2, max_trg)\n",
        "        src_tensor = torch.full((B, max_s), src_pad, dtype=torch.long)\n",
        "        trg_tensor = torch.full((B, max_t), trg_pad, dtype=torch.long)\n",
        "        for i,(s,t) in enumerate(zip(srcs,trgs)):\n",
        "            ls = min(len(s), max_s)\n",
        "            lt = min(len(t), max_t-2)\n",
        "            if ls>0:\n",
        "                src_tensor[i,:ls] = torch.tensor(s[:ls], dtype=torch.long)\n",
        "            seq = [trg_sos] + t[:lt] + [trg_eos]\n",
        "            trg_tensor[i,:len(seq)] = torch.tensor(seq, dtype=torch.long)\n",
        "        return src_tensor.to(DEVICE), trg_tensor.to(DEVICE)\n",
        "    return collate\n"
      ],
      "metadata": {
        "id": "aix3SAMUd2UD"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Models\n",
        "# -----------------------------\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers=2, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=0)\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=n_layers, dropout=dropout, batch_first=True, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, src):\n",
        "        emb = self.dropout(self.embedding(src))\n",
        "        outputs, (h,c) = self.rnn(emb)\n",
        "        return outputs, (h,c)\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=0)\n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=n_layers, dropout=dropout, batch_first=True)\n",
        "        self.fc = nn.Linear(hid_dim, output_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "    def forward(self, token_in, hidden):\n",
        "        emb = self.dropout(self.embedding(token_in).unsqueeze(1))\n",
        "        out, hidden = self.rnn(emb, hidden)\n",
        "        logits = self.fc(out.squeeze(1))\n",
        "        return logits, hidden\n",
        "\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder: Encoder, decoder: Decoder, enc_hid, dec_hid):\n",
        "        super().__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.h_map = nn.Linear(enc_hid*2, dec_hid)\n",
        "        self.c_map = nn.Linear(enc_hid*2, dec_hid)\n",
        "    def _reduce(self, h):\n",
        "        n2,B,H = h.size()\n",
        "        n = n2 // 2\n",
        "        h = h.view(n, 2, B, H)\n",
        "        hcat = torch.cat([h[:,0,:,:], h[:,1,:,:]], dim=2)\n",
        "        mapped = torch.tanh(self.h_map(hcat))\n",
        "        return mapped\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        B = src.size(0)\n",
        "        T = trg.size(1)\n",
        "        V = self.decoder.fc.out_features\n",
        "        outputs = torch.zeros(B, T, V, device=src.device)\n",
        "        enc_out, (h,c) = self.encoder(src)\n",
        "        dec_h = self._reduce(h)\n",
        "        dec_c = self._reduce(c)\n",
        "        dec_need = self.decoder.rnn.num_layers\n",
        "        if dec_h.size(0) < dec_need:\n",
        "            last_h = dec_h[-1:].repeat(dec_need - dec_h.size(0), 1, 1)\n",
        "            dec_h = torch.cat([dec_h, last_h], dim=0)\n",
        "            last_c = dec_c[-1:].repeat(dec_need - dec_c.size(0), 1, 1)\n",
        "            dec_c = torch.cat([dec_c, last_c], dim=0)\n",
        "        hidden = (dec_h.contiguous(), dec_c.contiguous())\n",
        "        input_tok = trg[:,0]\n",
        "        for t in range(1, T):\n",
        "            logits, hidden = self.decoder(input_tok, hidden)\n",
        "            outputs[:,t] = logits\n",
        "            top1 = logits.argmax(1)\n",
        "            teacher_force = (random.random() < teacher_forcing_ratio)\n",
        "            input_tok = trg[:,t] if teacher_force else top1\n",
        "        return outputs\n"
      ],
      "metadata": {
        "id": "bGnCyhFYeEtj"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Metrics: BLEU (sentence-level) & CER\n",
        "# -----------------------------\n",
        "def ngram_counts(tokens, n):\n",
        "    return Counter(tuple(tokens[i:i+n]) for i in range(len(tokens)-n+1)) if len(tokens)>=n else Counter()\n",
        "\n",
        "def sentence_bleu(ref_tokens, cand_tokens, max_n=4):\n",
        "    precisions = []\n",
        "    for n in range(1, max_n+1):\n",
        "        ref_cnt = ngram_counts(ref_tokens, n)\n",
        "        cand_cnt = ngram_counts(cand_tokens, n)\n",
        "        overlap = sum(min(cand_cnt[ng], ref_cnt.get(ng,0)) for ng in cand_cnt)\n",
        "        total = sum(cand_cnt.values())\n",
        "        p = 0.0 if total == 0 else overlap/total\n",
        "        precisions.append(p)\n",
        "    if min(precisions) == 0:\n",
        "        geo = 0.0\n",
        "    else:\n",
        "        geo = math.exp(sum(math.log(p) for p in precisions)/len(precisions))\n",
        "    rlen = len(ref_tokens); clen = len(cand_tokens)\n",
        "    if clen == 0:\n",
        "        bp = 0.0\n",
        "    else:\n",
        "        bp = 1.0 if clen > rlen else math.exp(1 - rlen/clen)\n",
        "    return bp * geo\n",
        "\n",
        "def cer(ref, hyp):\n",
        "    a=list(ref); b=list(hyp)\n",
        "    n=len(a); m=len(b)\n",
        "    if n==0:\n",
        "        return float(m)\n",
        "    dp=[[0]*(m+1) for _ in range(n+1)]\n",
        "    for i in range(n+1): dp[i][0]=i\n",
        "    for j in range(m+1): dp[0][j]=j\n",
        "    for i in range(1,n+1):\n",
        "        for j in range(1,m+1):\n",
        "            cost = 0 if a[i-1]==b[j-1] else 1\n",
        "            dp[i][j] = min(dp[i-1][j]+1, dp[i][j-1]+1, dp[i-1][j-1]+cost)\n",
        "    return dp[n][m]/max(1,n)"
      ],
      "metadata": {
        "id": "QEvHnpiSeLnO"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Training & evaluation\n",
        "# -----------------------------\n",
        "def train_epoch(model, loader, optimizer, criterion, clip, tf_ratio):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for src, trg in loader:\n",
        "        optimizer.zero_grad()\n",
        "        out = model(src, trg, teacher_forcing_ratio=tf_ratio)\n",
        "        V = out.shape[-1]\n",
        "        out_flat = out[:,1:,:].contiguous().view(-1, V)\n",
        "        trg_flat = trg[:,1:].contiguous().view(-1)\n",
        "        loss = criterion(out_flat, trg_flat)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / max(1, len(loader))\n",
        "\n",
        "def evaluate(model, loader, criterion, trg_rev_vocab):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    refs=[]; hyps=[]\n",
        "    with torch.no_grad():\n",
        "        for src, trg in loader:\n",
        "            outputs = model(src, trg, teacher_forcing_ratio=0.0)\n",
        "            V = outputs.shape[-1]\n",
        "            out_flat = outputs[:,1:,:].contiguous().view(-1, V)\n",
        "            trg_flat = trg[:,1:].contiguous().view(-1)\n",
        "            loss = criterion(out_flat, trg_flat)\n",
        "            total_loss += loss.item()\n",
        "            B = src.size(0)\n",
        "            for i in range(B):\n",
        "                # reference\n",
        "                ref_ids = trg[i].cpu().tolist()\n",
        "                ref_tokens=[]\n",
        "                for idx in ref_ids:\n",
        "                    if idx==trg_pad or idx==trg_sos:\n",
        "                        continue\n",
        "                    if idx==trg_eos:\n",
        "                        break\n",
        "                    ref_tokens.append(trg_rev_vocab.get(idx,'<unk>'))\n",
        "                refs.append(' '.join(ref_tokens))\n",
        "                # hypothesis from outputs\n",
        "                hyp_ids = outputs[i].argmax(1).cpu().tolist()\n",
        "                hyp_tokens=[]\n",
        "                for idx in hyp_ids:\n",
        "                    if idx==trg_pad or idx==trg_sos:\n",
        "                        continue\n",
        "                    if idx==trg_eos:\n",
        "                        break\n",
        "                    hyp_tokens.append(trg_rev_vocab.get(idx,'<unk>'))\n",
        "                hyps.append(' '.join(hyp_tokens))\n",
        "    avg_loss = total_loss / max(1, len(loader))\n",
        "    return avg_loss, refs, hyps\n"
      ],
      "metadata": {
        "id": "woAMOqMXeSi0"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -----------------------------\n",
        "# Main pipeline\n",
        "# -----------------------------\n",
        "def main():\n",
        "    start_time = time.time()\n",
        "    # prepare dataset extraction\n",
        "    extracted_root = prepare_dataset_extraction()\n",
        "    if extracted_root is None:\n",
        "        print(\"Could not prepare dataset. Put dataset.zip in one of candidate paths or enable cloning.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # try to find dataset root folder containing 'dataset' subfolder\n",
        "    dataset_root = None\n",
        "    # common structure: some zips extract to \"urdu_ghazals_rekhta-main/dataset\" or \"urdu_ghazals_rekhta/dataset\"\n",
        "    for cand in [os.path.join(extracted_root,\"dataset\"), os.path.join(extracted_root,\"urdu_ghazals_rekhta\",\"dataset\"), extracted_root]:\n",
        "        if os.path.isdir(cand):\n",
        "            # if it's the top-level extracted dir, check children for 'dataset'\n",
        "            if os.path.basename(cand).lower() == \"dataset\" or any('dataset' in d.lower() for d in os.listdir(os.path.dirname(cand)) if os.path.isdir(os.path.dirname(cand))):\n",
        "                dataset_root = cand\n",
        "                break\n",
        "            else:\n",
        "                dataset_root = cand\n",
        "                break\n",
        "    if dataset_root is None:\n",
        "        dataset_root = extracted_root\n",
        "    print(\"Using dataset root:\", dataset_root)\n",
        "\n",
        "    # discover Urdu lines\n",
        "    urdu_lines = discover_urdu_lines(dataset_root)\n",
        "    if not urdu_lines:\n",
        "        print(\"No Urdu lines discovered. Inspect dataset directory and try again.\")\n",
        "        sys.exit(1)\n",
        "\n",
        "    # LIMIT for speed (optional). For now use all lines.\n",
        "    print(\"Total raw Urdu lines found:\", len(urdu_lines))\n",
        "\n",
        "    # generate roman targets (fully generated approach)\n",
        "    pairs = []\n",
        "    for u in urdu_lines:\n",
        "        u_clean = clean_urdu(u)\n",
        "        roman = romanize_urdu(u_clean)\n",
        "        if roman and u_clean:\n",
        "            pairs.append((u_clean.lower(), roman.lower()))\n",
        "\n",
        "    print(\"Total synthetic pairs generated:\", len(pairs))\n",
        "    if len(pairs) < 20:\n",
        "        print(\"Warning: too few synthetic pairs:\", len(pairs))\n",
        "\n",
        "    # shuffle and split 50/25/25\n",
        "    random.shuffle(pairs)\n",
        "    N = len(pairs)\n",
        "    n_train = int(0.5 * N)\n",
        "    n_val = int(0.25 * N)\n",
        "    train_pairs = pairs[:n_train]\n",
        "    val_pairs = pairs[n_train:n_train+n_val]\n",
        "    test_pairs = pairs[n_train+n_val:]\n",
        "    print(\"Splits (train/val/test):\", len(train_pairs), len(val_pairs), len(test_pairs))\n",
        "\n",
        "    # learn BPE on train side (both source and target)\n",
        "    print(\"Learning BPE (source)...\")\n",
        "    src_texts = [s for s,t in train_pairs]\n",
        "    trg_texts = [t for s,t in train_pairs]\n",
        "    src_bpe = BPE(target_vocab_size=cfg['bpe_vocab'], min_freq=cfg.get('min_bpe_freq',2))\n",
        "    trg_bpe = BPE(target_vocab_size=cfg['bpe_vocab'], min_freq=cfg.get('min_bpe_freq',2))\n",
        "    src_bpe.learn(src_texts)\n",
        "    trg_bpe.learn(trg_texts)\n",
        "    print(\"Source vocab size:\", len(src_bpe.vocab), \"Target vocab size:\", len(trg_bpe.vocab))\n",
        "\n",
        "    # prepare datasets & dataloaders\n",
        "    global trg_pad, trg_sos, trg_eos\n",
        "    trg_pad = trg_bpe.vocab['<pad>']; trg_sos = trg_bpe.vocab['<sos>']; trg_eos = trg_bpe.vocab['<eos>']\n",
        "    src_pad = src_bpe.vocab['<pad>']\n",
        "\n",
        "    train_ds = ParallelDataset(train_pairs, src_bpe, trg_bpe, max_src=cfg['max_src_len'], max_trg=cfg['max_trg_len'])\n",
        "    val_ds = ParallelDataset(val_pairs, src_bpe, trg_bpe, max_src=cfg['max_src_len'], max_trg=cfg['max_trg_len'])\n",
        "    test_ds = ParallelDataset(test_pairs, src_bpe, trg_bpe, max_src=cfg['max_src_len'], max_trg=cfg['max_trg_len'])\n",
        "\n",
        "    collate = collate_factory(src_pad, trg_pad, trg_sos, trg_eos, cfg['max_src_len'], cfg['max_trg_len'])\n",
        "    train_loader = DataLoader(train_ds, batch_size=cfg['batch_size'], shuffle=True, collate_fn=collate)\n",
        "    val_loader = DataLoader(val_ds, batch_size=cfg['batch_size'], shuffle=False, collate_fn=collate)\n",
        "    test_loader = DataLoader(test_ds, batch_size=cfg['batch_size'], shuffle=False, collate_fn=collate)\n",
        "\n",
        "    # build model\n",
        "    encoder = Encoder(len(src_bpe.vocab), cfg['emb_dim'], cfg['enc_hidden'], n_layers=cfg['enc_layers'], dropout=cfg['dropout']).to(DEVICE)\n",
        "    decoder = Decoder(len(trg_bpe.vocab), cfg['emb_dim'], cfg['dec_hidden'], n_layers=cfg['dec_layers'], dropout=cfg['dropout']).to(DEVICE)\n",
        "    model = Seq2Seq(encoder, decoder, cfg['enc_hidden'], cfg['dec_hidden']).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=cfg['lr'], weight_decay=cfg.get('weight_decay',0.0))\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=trg_pad)\n",
        "\n",
        "    print(\"Training on device:\", DEVICE)\n",
        "    best_val = float('inf'); best_state=None; epochs_no_improve=0\n",
        "    for epoch in range(1, cfg['epochs']+1):\n",
        "        # linear teacher forcing anneal\n",
        "        if cfg['epochs']>1:\n",
        "            frac = (epoch-1)/(cfg['epochs']-1)\n",
        "            tf = cfg['teacher_forcing_start'] + (cfg['teacher_forcing_end'] - cfg['teacher_forcing_start']) * frac\n",
        "        else:\n",
        "            tf = cfg['teacher_forcing_end']\n",
        "        t0 = time.time()\n",
        "        train_loss = train_epoch(model, train_loader, optimizer, criterion, cfg['grad_clip'], tf)\n",
        "        val_loss, val_refs, val_hyps = evaluate(model, val_loader, criterion, trg_bpe.rev)\n",
        "        t1 = time.time()\n",
        "        # compute val CER and BLEU averages (sentence-level average)\n",
        "        val_bleu = 0.0\n",
        "        val_cer = 0.0\n",
        "        if val_refs:\n",
        "            val_bleu = sum(sentence_bleu(r.split(), h.split()) for r,h in zip(val_refs, val_hyps)) / len(val_refs)\n",
        "            val_cer = sum(cer(r.replace(' ','') , h.replace(' ','')) for r,h in zip(val_refs, val_hyps)) / len(val_refs)\n",
        "        print(f\"[{cfg['name']}] Epoch {epoch}/{cfg['epochs']} TrainLoss={train_loss:.4f} ValLoss={val_loss:.4f} ValBLEU={val_bleu:.4f} ValCER={val_cer:.4f} TF={tf:.3f} time={t1-t0:.1f}s\")\n",
        "        if val_loss < best_val - 1e-6:\n",
        "            best_val = val_loss\n",
        "            best_state = copy.deepcopy(model.state_dict())\n",
        "            torch.save(best_state, f\"{cfg['name']}_best.pt\")\n",
        "            print(\"Saved checkpoint:\", f\"{cfg['name']}_best.pt\")\n",
        "            epochs_no_improve = 0\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= cfg['patience']:\n",
        "                print(\"Early stopping.\")\n",
        "                break\n",
        "\n",
        "    # final evaluation on test\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    test_loss, test_refs, test_hyps = evaluate(model, test_loader, criterion, trg_bpe.rev)\n",
        "    test_bleu = 0.0; test_cer=0.0\n",
        "    if test_refs:\n",
        "        test_bleu = sum(sentence_bleu(r.split(), h.split()) for r,h in zip(test_refs, test_hyps)) / len(test_refs)\n",
        "        test_cer = sum(cer(r.replace(' ','') , h.replace(' ','')) for r,h in zip(test_refs, test_hyps)) / len(test_refs)\n",
        "    ppl = math.exp(test_loss) if test_loss < 100 else float('inf')\n",
        "\n",
        "    print(\"\\n=== TEST RESULTS ===\")\n",
        "    print(\"Test loss:\", test_loss)\n",
        "    print(\"Test Perplexity:\", ppl)\n",
        "    print(\"Test BLEU:\", test_bleu)\n",
        "    print(\"Test CER:\", test_cer)\n",
        "\n",
        "    # qualitative examples: show a few source -> expected (generated) -> predicted\n",
        "    n_show = min(12, len(test_refs))\n",
        "    print(\"\\n--- Sample Predictions ---\")\n",
        "    for i in range(n_show):\n",
        "        src_line = test_pairs[i][0]\n",
        "        ref_line = test_refs[i]\n",
        "        hyp_line = test_hyps[i]\n",
        "        print(\"Input: \", src_line)\n",
        "        print(\"True:  \", ref_line)\n",
        "        print(\"Pred:  \", hyp_line)\n",
        "        print(\"CER:   \", round(cer(ref_line.replace(' ','') , hyp_line.replace(' ','')), 4))\n",
        "        print(\"-\"*40)\n",
        "\n",
        "    total_time = time.time() - start_time\n",
        "    print(\"Total time (s):\", round(total_time,1))\n",
        "    # ===============================\n",
        "    # Save model and BPE tokenizers\n",
        "    # ===============================\n",
        "    torch.save(model.state_dict(), \"exp_small_best.pt\")\n",
        "    torch.save(src_bpe, \"src_bpe.pkl\")\n",
        "    torch.save(trg_bpe, \"trg_bpe.pkl\")\n",
        "\n",
        "    print(\"\\n✅ Model and tokenizers saved successfully!\")\n",
        "    print(\"Files created:\")\n",
        "    print(\" - exp_small_best.pt\")\n",
        "    print(\" - src_bpe.pkl\")\n",
        "    print(\" - trg_bpe.pkl\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHF-dz5TpBmC",
        "outputId": "3bdb881b-bb30-49b2-b4a4-738da915af62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted zip: ./dataset.zip -> ./_rekhta_extracted\n",
            "Using dataset root: ./_rekhta_extracted/dataset\n",
            "Found 21068 Urdu lines from author/ur folders.\n",
            "Total raw Urdu lines found: 21068\n",
            "Total synthetic pairs generated: 21068\n",
            "Splits (train/val/test): 10534 5267 5267\n",
            "Learning BPE (source)...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from train_bilstm_transliteration_complete import Encoder, Decoder, Seq2Seq, BPE\n",
        "import torch\n",
        "\n",
        "# Rebuild model & BPEs with same parameters\n",
        "src_bpe = torch.load(\"src_bpe.pkl\")\n",
        "trg_bpe = torch.load(\"trg_bpe.pkl\")\n",
        "\n",
        "src_vocab_size = len(src_bpe.vocab)\n",
        "trg_vocab_size = len(trg_bpe.vocab)\n",
        "\n",
        "model = Seq2Seq(\n",
        "    Encoder(src_vocab_size, 256, 512, n_layers=2, dropout=0.3),\n",
        "    Decoder(trg_vocab_size, 256, 512, n_layers=4, dropout=0.3),\n",
        "    512\n",
        ")\n",
        "model.load_state_dict(torch.load(\"exp_small_best.pt\", map_location='cpu'))\n",
        "\n",
        "# Re-save (optional cleanup)\n",
        "torch.save(model.state_dict(), \"exp_small_best.pt\")\n",
        "torch.save(src_bpe, \"src_bpe.pkl\")\n",
        "torch.save(trg_bpe, \"trg_bpe.pkl\")\n",
        "\n",
        "print(\"✅ Re-saved all files successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "ayhb9UMylAcM",
        "outputId": "078b0c49-ca64-4328-c6b0-d14b3b8f0673"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'train_bilstm_transliteration_complete'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-473247093.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtrain_bilstm_transliteration_complete\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEncoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeq2Seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBPE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Rebuild model & BPEs with same parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msrc_bpe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"src_bpe.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'train_bilstm_transliteration_complete'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNZXKB_0l86D",
        "outputId": "b2fad52e-9a42-447f-abb8-a92ee7ddbc31"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset      exp_small_best.pt\t_rekhta_extracted  urdu_ghazals_rekhta\n",
            "dataset.zip  __MACOSX\t\tsample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "koSUUViiStFm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}